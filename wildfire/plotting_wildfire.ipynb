{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51c4f4ef",
   "metadata": {},
   "source": [
    "Plotting validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3809d73c",
   "metadata": {},
   "source": [
    "Plotting testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02eb4b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os, math\n",
    "\n",
    "policy_eval_outputs = {\n",
    "    1: 'results/OL1_frank_no_drop/testing_eval',\n",
    "    2: 'results/OL2_frank_no_drop/testing_eval',\n",
    "    3: 'results/OL3_frank_no_drop/testing_eval',\n",
    "\n",
    "    -1: 'results/OL1_frank_no_drop_ablation/testing_eval',\n",
    "    -2: 'results/OL2_frank_no_drop_ablation/testing_eval',\n",
    "    -3: 'results/OL3_frank_no_drop_ablation/testing_eval',\n",
    "}\n",
    "\n",
    "baseline_output_folder = 'baseline_output'\n",
    "\n",
    "policy_best_checkpoints = {\n",
    "    1: {},\n",
    "    2: {},\n",
    "    3: {},\n",
    "    -1: {},\n",
    "    -2: {},\n",
    "    -3: {},\n",
    "}\n",
    "\n",
    "for openness_level, file_path in policy_eval_outputs.items():\n",
    "\n",
    "    dfs = [\n",
    "        (pd.read_csv(os.path.join(root, file)), root.split('/')[-1])\n",
    "        for root, _, files in os.walk(file_path)\n",
    "        for file in files if file.endswith('.csv')\n",
    "    ]\n",
    "\n",
    "    dfs = pd.concat([df.assign(policy=file.split(';')[1].split('_')[0]) for df, file in dfs], ignore_index=True)\n",
    "\n",
    "    reward_cols = [col for col in dfs.columns if 'rewards' in col]\n",
    "\n",
    "    # dfs['policy'] = dfs['description'].apply(lambda x: x.split('_')[0].split(';')[1])\n",
    "    dfs['openness level'] = openness_level\n",
    "    dfs['starting state'] = dfs['description'].apply(lambda x: int(x.split('_')[2].split(';')[1]))\n",
    "    dfs['episodes'] = dfs['description'].apply(lambda x: int(x.split('_')[3].split(';')[1]))\n",
    "\n",
    "    original_dfs = dfs.copy()\n",
    "\n",
    "    dfs = dfs[['description', 'step', 'policy', 'openness level','starting state', 'episodes'] + reward_cols]\n",
    "    dfs.drop(columns=['description'], inplace=True)\n",
    "\n",
    "    #sum over steps\n",
    "    dfs = dfs.groupby(['policy', 'openness level', 'episodes', 'starting state']).sum().reset_index()\n",
    "    dfs['final_rewards'] = dfs[reward_cols].mean(axis=1)\n",
    "\n",
    "\n",
    "    #average over starting states / episodes generally speaking\n",
    "    pivot = pd.pivot_table(dfs, index=['openness level'], columns=['policy'], values='final_rewards', aggfunc='mean')\n",
    "    best_policy = pivot.idxmax(axis=1)\n",
    "    pivot = pivot[best_policy]\n",
    "\n",
    "\n",
    "    pivot_std = pd.pivot_table(dfs, index=['openness level'], columns=['policy'], values='final_rewards', aggfunc='std')\n",
    "    pivot_std = 2.04523 * pivot_std / math.sqrt(60)\n",
    "    pivot_std = pivot_std[best_policy]\n",
    "\n",
    "    policy_best_checkpoints[openness_level]['mean'] = pivot\n",
    "    policy_best_checkpoints[openness_level]['ci'] = pivot_std\n",
    "    policy_best_checkpoints[openness_level]['checkpoint'] = best_policy\n",
    "    policy_best_checkpoints[openness_level]['dfs'] = original_dfs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c5d90b0",
   "metadata": {},
   "source": [
    "Loading baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ca62ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "dfs = pd.concat([\n",
    "    pd.read_csv(os.path.join(root, file))\n",
    "    for root, _, files in os.walk(baseline_output_folder) for file in files\n",
    "    if file.endswith('.csv')\n",
    "], ignore_index=True)\n",
    "\n",
    "reward_cols = [col for col in dfs.columns if 'rewards' in col]\n",
    "\n",
    "dfs = dfs[['description', 'step'] + reward_cols]\n",
    "dfs['policy'] = dfs['description'].apply(\n",
    "    lambda x: x.split('_')[0].split(';')[1])\n",
    "dfs['openness level'] = dfs['description'].apply(\n",
    "    lambda x: int(x.split('_')[1].split(';')[1]))\n",
    "dfs['starting state'] = dfs['description'].apply(\n",
    "    lambda x: int(x.split('_')[2].split(';')[1]))\n",
    "dfs['episodes'] = dfs['description'].apply(\n",
    "    lambda x: int(x.split('_')[3].split(';')[1]))\n",
    "dfs.drop(columns=['description'], inplace=True)\n",
    "\n",
    "#sum over steps\n",
    "dfs = dfs.groupby(['policy', 'openness level', 'episodes',\n",
    "                   'starting state']).sum().reset_index()\n",
    "dfs['final_rewards'] = dfs[reward_cols].mean(axis=1)\n",
    "\n",
    "#average over starting states / episodes generally speaking\n",
    "pivot = pd.pivot_table(dfs,\n",
    "                       index=['openness level'],\n",
    "                       columns=['policy'],\n",
    "                       values='final_rewards',\n",
    "                       aggfunc='mean')\n",
    "\n",
    "pivot_std = pd.pivot_table(dfs,\n",
    "                           index=['openness level'],\n",
    "                           columns=['policy'],\n",
    "                           values='final_rewards',\n",
    "                           aggfunc='std')\n",
    "pivot_std = 2.04523 * pivot_std / math.sqrt(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886bdc9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mohito_best_policy_df = pd.concat([\n",
    "    policy_best_checkpoints[openness_level]['mean']\n",
    "    for openness_level in policy_best_checkpoints\n",
    "])\n",
    "\n",
    "print(mohito_best_policy_df)\n",
    "\n",
    "#merge MOHITOX columns of nonNaNs into one column\n",
    "mohito_best_policy_df = mohito_best_policy_df.reset_index()\n",
    "mohito_best_policy_df = mohito_best_policy_df.melt(\n",
    "    id_vars=['openness level'],\n",
    "    var_name='policy',\n",
    "    value_name='final_rewards'\n",
    ").dropna()\n",
    "\n",
    "mohito_best_policy_df['policy'] = 'MOHITO'\n",
    "mohito_best_policy_df.set_index('openness level', inplace=True)\n",
    "\n",
    "#incorporate ablation results\n",
    "mohito_original_policy_df = mohito_best_policy_df[mohito_best_policy_df.index > 0]\n",
    "mohito_ablation_df = mohito_best_policy_df[mohito_best_policy_df.index < 0]\n",
    "\n",
    "#flip\n",
    "mohito_ablation_df.index = -mohito_ablation_df.index\n",
    "mohito_ablation_df['policy'] = mohito_ablation_df['policy'] + ' (Ablation)'\n",
    "mohito_best_policy_df = pd.concat([mohito_original_policy_df, mohito_ablation_df])\n",
    "\n",
    "\n",
    "print(mohito_best_policy_df)\n",
    "\n",
    "# Merge MOHITO and baseline dataframes\n",
    "pivot['mohito'] = mohito_best_policy_df['final_rewards'][mohito_best_policy_df['policy'] == 'MOHITO']\n",
    "pivot['mohito (Ablation)'] = mohito_best_policy_df['final_rewards'][mohito_best_policy_df['policy'] == 'MOHITO (Ablation)']\n",
    "print(pivot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "025860c6",
   "metadata": {},
   "source": [
    "and ci"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cfa3b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "mohito_best_std_df = pd.concat([\n",
    "    policy_best_checkpoints[openness_level]['ci']\n",
    "    for openness_level in policy_best_checkpoints\n",
    "])\n",
    "\n",
    "print(mohito_best_std_df)\n",
    "\n",
    "#merge MOHITOX columns of nonNaNs into one column\n",
    "mohito_best_std_df = mohito_best_std_df.reset_index()\n",
    "mohito_best_std_df = mohito_best_std_df.melt(\n",
    "    id_vars=['openness level'],\n",
    "    var_name='policy',\n",
    "    value_name='final_rewards'\n",
    ").dropna()\n",
    "\n",
    "mohito_best_std_df['policy'] = 'MOHITO'\n",
    "mohito_best_std_df.set_index('openness level', inplace=True)\n",
    "\n",
    "\n",
    "#incorporate ablation results\n",
    "mohito_original_std_df = mohito_best_std_df[mohito_best_std_df.index > 0]\n",
    "mohito_ablation_df = mohito_best_std_df[mohito_best_std_df.index < 0]\n",
    "\n",
    "#flip\n",
    "mohito_ablation_df.index = -mohito_ablation_df.index\n",
    "mohito_ablation_df['policy'] = mohito_ablation_df['policy'] + ' (Ablation)'\n",
    "mohito_best_std_df = pd.concat([mohito_original_std_df, mohito_ablation_df])\n",
    "\n",
    "\n",
    "\n",
    "print(mohito_best_std_df)\n",
    "\n",
    "# Merge MOHITO and baseline dataframes\n",
    "pivot_std['mohito'] = mohito_best_std_df['final_rewards'][mohito_best_std_df['policy'] == 'MOHITO']\n",
    "pivot_std['mohito (Ablation)'] = mohito_best_std_df['final_rewards'][mohito_best_std_df['policy'] == 'MOHITO (Ablation)']\n",
    "print(pivot_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "502fb2ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = ['#009ADE','red','#AF58BA','#FFC61E', '#F28522']\n",
    "\n",
    "print(pivot.columns)\n",
    "\n",
    "pivot.rename(columns={'mohito': 'MOH', 'mohito (Ablation)':'MOH2d', 'FifoBaseline':'FCFS', 'WeakestBaseline': 'NTF', 'RandomBaseline': 'Random'}, inplace=True)\n",
    "pivot_std.rename(columns={'mohito': 'MOH', 'mohito (Ablation)':'MOH2d', 'FifoBaseline':'FCFS', 'WeakestBaseline': 'NTF', 'RandomBaseline': 'Random'}, inplace=True)\n",
    "\n",
    "#filter to just the renamed columns\n",
    "pivot = pivot[['MOH', 'MOH2d', 'FCFS', 'NTF', 'Random']]\n",
    "pivot_std = pivot_std[['MOH', 'MOH2d', 'FCFS', 'NTF', 'Random']]\n",
    "\n",
    "p1 = pivot.copy()\n",
    "p1_std = pivot_std.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ee4191",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "\n",
    "p1.plot(kind='bar',\n",
    "           yerr=p1_std,\n",
    "           capsize=5,\n",
    "           figsize=(7, 3),\n",
    "           title='Average Final Rewards by Openness Level and Policy',\n",
    "           color=colors,\n",
    "           width=0.8,)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.title('')\n",
    "plt.yticks(size=12)\n",
    "plt.xticks(size=12, rotation=0)\n",
    "plt.ylim(-380, 350)\n",
    "plt.xlabel('Openness Level', size=14)\n",
    "plt.ylabel('Mean Reward (with CI)', size=14)\n",
    "plt.legend(title=None, framealpha=0.0, fontsize=12, labels=[\"MOHITO\",\"MOHITO-NoTaskNodes\",\"FCFS\",\"NTF\", \"Random\"], ncol=3, prop={'size':10}, loc='lower left')\n",
    "plt.tight_layout()\n",
    "plt.savefig('wildfire_rewards.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "587bc74f",
   "metadata": {},
   "source": [
    "wilcoxon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f87753",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "#add MOHITO\n",
    "policy_eval_outputs = {\n",
    "    1: ('results/OL1_frank_no_drop/testing_eval', policy_best_checkpoints[1]['checkpoint']),\n",
    "    2: ('results/OL2_frank_no_drop/testing_eval', policy_best_checkpoints[2]['checkpoint']),\n",
    "    3: ('results/OL3_frank_no_drop/testing_eval', policy_best_checkpoints[3]['checkpoint']),\n",
    "\n",
    "    -1: ('results/OL1_frank_no_drop_ablation/testing_eval', policy_best_checkpoints[-1]['checkpoint']),\n",
    "    -2: ('results/OL2_frank_no_drop_ablation/testing_eval', policy_best_checkpoints[-2]['checkpoint']),\n",
    "    -3: ('results/OL3_frank_no_drop_ablation/testing_eval', policy_best_checkpoints[-3]['checkpoint']),\n",
    "}\n",
    "\n",
    "mohito_dfs = pd.concat([\n",
    "    policy_best_checkpoints[openness_level]['dfs'][policy_best_checkpoints[openness_level]['dfs']['policy'] == policy_best_checkpoints[openness_level]['checkpoint'].item()].copy()\n",
    "    for openness_level in policy_eval_outputs\n",
    "])\n",
    "\n",
    "\n",
    "#handle the ablation\n",
    "mohito_ablation_dfs = mohito_dfs['openness level'] < 0\n",
    "mohito_dfs['policy'] = 'mohito'\n",
    "mohito_ablation_policy_labels = mohito_dfs[mohito_ablation_dfs]['policy'].apply(lambda x: f'{x} (Ablation)')\n",
    "mohito_dfs.loc[mohito_ablation_dfs, 'policy'] = mohito_ablation_policy_labels\n",
    "mohito_dfs['openness level'] = mohito_dfs['openness level'].abs()\n",
    "\n",
    "#add MOHITO to the dfs\n",
    "dfs = pd.concat([dfs, mohito_dfs], ignore_index=True).copy()\n",
    "\n",
    "\n",
    "#sum over steps\n",
    "dfs = dfs.groupby(['policy', 'openness level', 'episodes',\n",
    "                   'starting state'])[reward_cols].sum().reset_index()\n",
    "\n",
    "dfs['final_rewards'] = dfs[reward_cols].mean(axis=1)\n",
    "\n",
    "#sort values\n",
    "dfs.sort_values(by=['policy','openness level','episodes', 'starting state'], inplace=True)\n",
    "\n",
    "dfg = dfs.groupby(['policy'])\n",
    "\n",
    "for (policy, group) in dfg:\n",
    "\n",
    "    if policy == 'mohito':\n",
    "        continue\n",
    "\n",
    "    print(f'Wilcoxon test: {policy} - MOHITO')\n",
    "    stat, p_value = stats.wilcoxon(\n",
    "        group['final_rewards'],\n",
    "        dfg.get_group('mohito')['final_rewards']\n",
    "    )\n",
    "    print(f'Statistic: {stat}, p-value: {p_value}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0759a7f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, os\n",
    "import pandas as pd\n",
    "from ast import literal_eval\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "dfs = pd.concat([\n",
    "    pd.read_csv(os.path.join(root, file))\n",
    "    for root, _, files in os.walk(baseline_output_folder) for file in files\n",
    "    if file.endswith('.csv')\n",
    "], ignore_index=True)\n",
    "\n",
    "#get groups\n",
    "dfs['policy'] = dfs['description'].apply(\n",
    "    lambda x: x.split('_')[0].split(';')[1])\n",
    "dfs['openness level'] = dfs['description'].apply(\n",
    "    lambda x: int(x.split('_')[1].split(';')[1]))\n",
    "dfs['starting state'] = dfs['description'].apply(\n",
    "    lambda x: int(x.split('_')[2].split(';')[1]))\n",
    "dfs['episodes'] = dfs['description'].apply(\n",
    "    lambda x: int(x.split('_')[3].split(';')[1]))\n",
    "dfs.drop(columns=['description'], inplace=True)\n",
    "\n",
    "#add MOHITO\n",
    "policy_eval_outputs = {\n",
    "    1: ('results/OL1_frank_no_drop/testing_eval', policy_best_checkpoints[1]['checkpoint']),\n",
    "    2: ('results/OL2_frank_no_drop/testing_eval', policy_best_checkpoints[2]['checkpoint']),\n",
    "    3: ('results/OL3_frank_no_drop/testing_eval', policy_best_checkpoints[3]['checkpoint']),\n",
    "\n",
    "    -1: ('results/OL1_frank_no_drop_ablation/testing_eval_perm', policy_best_checkpoints[-1]['checkpoint']),\n",
    "    -2: ('results/OL2_frank_no_drop_ablation/testing_eval_perm', policy_best_checkpoints[-2]['checkpoint']),\n",
    "    -3: ('results/OL3_frank_no_drop_ablation/testing_eval_perm', policy_best_checkpoints[-3]['checkpoint']),\n",
    "}\n",
    "\n",
    "mohito_dfs = pd.concat([\n",
    "    policy_best_checkpoints[openness_level]['dfs'][policy_best_checkpoints[openness_level]['dfs']['policy'] == policy_best_checkpoints[openness_level]['checkpoint'].item()].copy()\n",
    "    for openness_level in policy_eval_outputs\n",
    "])\n",
    "\n",
    "\n",
    "#handle the ablation\n",
    "mohito_ablation_dfs = mohito_dfs['openness level'] < 0\n",
    "mohito_dfs['policy'] = 'mohito'\n",
    "mohito_ablation_policy_labels = mohito_dfs[mohito_ablation_dfs]['policy'].apply(lambda x: f'{x} (Ablation)')\n",
    "mohito_dfs.loc[mohito_ablation_dfs, 'policy'] = mohito_ablation_policy_labels\n",
    "mohito_dfs['openness level'] = mohito_dfs['openness level'].abs()\n",
    "\n",
    "#add MOHITO to the dfs\n",
    "dfs = pd.concat([dfs, mohito_dfs], ignore_index=True)\n",
    "\n",
    "\n",
    "#get time column as np array\n",
    "dfs = dfs[['policy', 'step', 'openness level', 'episodes', 'starting state', 'infos/just_put_out_time', \n",
    "    'infos/just_put_out_ftype', 'infos/just_burned_out_time', 'infos/just_burned_out_ftype']]\n",
    "\n",
    "grouping = dfs.groupby(['policy', 'openness level', 'starting state', 'episodes'])\n",
    "\n",
    "\n",
    "arrayify = lambda x: np.array(literal_eval(x))\n",
    "\n",
    "\n",
    "grouped_times = {p:[] for p in dfs['policy'].unique()}\n",
    "\n",
    "\n",
    "# Iterate through policies and openness levels to get the times and types of put out and burned out fires\n",
    "for (name, group) in grouping:\n",
    "\n",
    "    try:\n",
    "        group['just_put_out_time'] = group['infos/just_put_out_time'].apply(arrayify)\n",
    "    except:\n",
    "        print('hh')\n",
    "\n",
    "    group['just_burned_out_time'] = group['infos/just_burned_out_time'].apply(arrayify)\n",
    "    group['just_put_out_ftype'] = group['infos/just_put_out_ftype'].apply(arrayify)\n",
    "    group['just_burned_out_ftype'] = group['infos/just_burned_out_ftype'].apply(arrayify)\n",
    "\n",
    "\n",
    "    #create stacked_cols\n",
    "    put_outs = pd.DataFrame({\n",
    "        'time': np.concatenate(group['just_put_out_time'].values),\n",
    "        'fire_type': np.concatenate(group['just_put_out_ftype'].values),\n",
    "    })\n",
    "    put_outs['burned_out'] = False\n",
    "\n",
    "    burn_outs = pd.DataFrame({\n",
    "        'time': np.concatenate(group['just_burned_out_time'].values),\n",
    "        'fire_type': np.concatenate(group['just_burned_out_ftype'].values),\n",
    "    })\n",
    "    burn_outs['burned_out'] = True\n",
    "\n",
    "    stacked_cols = pd.concat([put_outs, burn_outs], ignore_index=True)  \n",
    "\n",
    "    stacked_cols['policy'] = name[0]\n",
    "    stacked_cols['openness_level'] = name[1]\n",
    "    stacked_cols['starting_state'] = name[2]\n",
    "    stacked_cols['episodes'] = name[3]\n",
    "    grouped_times[name[0]].append(stacked_cols)\n",
    "\n",
    "\n",
    "grouped_times = {\n",
    "    policy: pd.concat(times, ignore_index=True)\n",
    "    for policy, times in grouped_times.items()\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b94d2322",
   "metadata": {},
   "source": [
    "plot time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e4ecc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "gdf = pd.concat(grouped_times.values(), ignore_index=True)\n",
    "\n",
    "gdf = gdf[gdf['policy'] != 'NoopBaseline']\n",
    "gdf = gdf[['time', 'openness_level', 'policy']]\n",
    "\n",
    "print(gdf['policy'].unique())\n",
    "\n",
    "pivot = pd.pivot_table(gdf,\n",
    "    index = 'openness_level',\n",
    "    columns = 'policy',\n",
    "    values = 'time',\n",
    "    aggfunc='mean'\n",
    ")\n",
    "\n",
    "pivot_std = pd.pivot_table(gdf,\n",
    "    index = 'openness_level',\n",
    "    columns = 'policy',\n",
    "    values = 'time',\n",
    "    aggfunc='std'\n",
    ")\n",
    "pivot_std = 2.04523 * pivot_std / math.sqrt(60)\n",
    "\n",
    "pivot.rename(columns={'FifoBaseline': 'FCFS', 'WeakestBaseline': 'NTF', 'RandomBaseline': 'Random', 'mohito': 'MOH', 'mohito (Ablation)': 'MOH-NoTaskNodes'}, inplace=True)\n",
    "pivot_std.rename(columns={'FifoBaseline': 'FCFS', 'WeakestBaseline': 'NTF', 'RandomBaseline': 'Random', 'mohito': 'MOH', 'mohito (Ablation)': 'MOH-NoTaskNodes'}, inplace=True)\n",
    "\n",
    "\n",
    "pivot = pivot.reindex(columns=['MOH', 'MOH-NoTaskNodes', 'FCFS', 'NTF', 'Random'], fill_value=0)\n",
    "pivot_std = pivot_std.reindex(columns=['MOH', 'MOH-NoTaskNodes', 'FCFS', 'NTF', 'Random'], fill_value=0)\n",
    "\n",
    "pivot.plot(kind='bar',\n",
    "           yerr=pivot_std,\n",
    "           capsize=5,\n",
    "           figsize=(7, 4),\n",
    "           width=.8,\n",
    "           title='Average Time to Put Out Fires by Openness Level and Policy',\n",
    "           color=colors)\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.title('')\n",
    "plt.legend(title=None, framealpha=0.0, fontsize=12, labels=[\"MOHITO\", \"MOHITO-NoTaskNodes\", \"FCFS\",\"NTF\", \"Random\"], ncol=3, prop={'size':12}, loc='upper left')\n",
    "plt.yticks(size=14)\n",
    "plt.xticks(size=14, rotation=0)\n",
    "plt.ylim(0, 10)\n",
    "plt.ylabel(\"Fire Duration (in timesteps)\", size=14)\n",
    "plt.xlabel(\"Openness Level\",size=16)\n",
    "plt.tight_layout()\n",
    "plt.savefig('wildfire_duration.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9470c5eb",
   "metadata": {},
   "source": [
    "Plot the burn/put outs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fcf8758",
   "metadata": {},
   "outputs": [],
   "source": [
    "counted_grouped_times = pd.concat([\n",
    "    g.groupby(['policy','openness_level', 'burned_out', 'episodes', 'starting_state', 'fire_type'])['time'].count().reset_index()\n",
    "    for policy, g in grouped_times.items()\n",
    "])\n",
    "\n",
    "for is_burned_out in [True, False]:\n",
    "\n",
    "    these_grouped_times = counted_grouped_times[counted_grouped_times['burned_out'] == is_burned_out]\n",
    "\n",
    "    pivot = pd.pivot_table(these_grouped_times,\n",
    "        index= ['fire_type', 'openness_level'],\n",
    "        columns=['policy'],\n",
    "        values='time',\n",
    "        aggfunc='mean'\n",
    "    )\n",
    "    pivot_std = pd.pivot_table(these_grouped_times,\n",
    "        index= ['fire_type', 'openness_level'],\n",
    "        columns=['policy'],\n",
    "        values='time',\n",
    "        aggfunc='std'\n",
    "    )\n",
    "    pivot_std = 2.04523 * pivot_std / math.sqrt(60)\n",
    "\n",
    "    pivot.rename(columns={'FifoBaseline': 'FCFS', 'WeakestBaseline': 'NTF', 'RandomBaseline': 'Random', 'mohito': 'MOHITO', 'mohito (Ablation)': 'MOHITO-NoTaskNodes'}, inplace=True)\n",
    "    pivot_std.rename(columns={'FifoBaseline': 'FCFS', 'WeakestBaseline': 'NTF', 'RandomBaseline': 'Random', 'mohito': 'MOHITO', 'mohito (Ablation)': 'MOHITO-NoTaskNodes'}, inplace=True)\n",
    "\n",
    "    pivot = pivot.reindex(columns=['MOHITO', 'MOHITO-NoTaskNodes', 'FCFS', 'NTF', 'Random'], fill_value=0)\n",
    "\n",
    "    pivot.plot(kind='bar',\n",
    "            yerr=pivot_std,\n",
    "            capsize=5,\n",
    "            figsize=(7, 4),\n",
    "            width=.8,\n",
    "            title='',\n",
    "            color=colors)\n",
    "\n",
    "    plt.ylabel(f\"Number of Fires {'Burned Out' if is_burned_out else 'Put Out'}\", size=14)\n",
    "    plt.xlabel(\"Openness Level\", size=14)\n",
    "    locs = plt.xticks()[0]\n",
    "    plt.xticks(size=14, ticks=locs, labels=['1', '2', '3', '1', '2', '3'], rotation=0)\n",
    "    plt.legend(title=None, framealpha=0.0, fontsize=12, labels=[\"MOHITO\",\"MOHITO-NoTaskNodes\",\"FCFS\",\"NTF\", \"Random\"], ncol=1, prop={'size':10})\n",
    "    plt.axvline(x=2.5, color='black', linestyle='--', linewidth=2, label='_nolegend_')\n",
    "\n",
    "    plt.text(1, 0.03, 'Small Fires', size=14, va='bottom', ha='center',bbox=dict(facecolor='white', edgecolor='black', boxstyle='round,pad=0.1'))\n",
    "    plt.text(4, 0.03, 'Medium Fires', size=14, va='bottom', ha='center',bbox=dict(facecolor='white', edgecolor='black', boxstyle='round,pad=0.1'))\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.savefig(f'wildfire_fires_{is_burned_out}.pdf')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "three12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
