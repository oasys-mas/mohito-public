#parameters outside the model
training:
  seed: 16
  experiences_before_update: 1
  backups_per_update: 1
  num_episodes: 9000
  steps_per_checkpoint: 40
  batch_size: 16
  buffer_size: 1000
  wait_until_full: true
  starting_state_strategy: 'random'
  starting_states: [0,1,2]
  reprod_training: true
  terminate_on_no_fires: true
  intensity_increase_prob: 0.8

#convergence criteria
convergence:
  k_convergence: 5 #number of sequential runs to consider for convergence
  convergence_diff_bound: 15 #if the difference in k rewards are all less than or equal to the current reward, we have converged
  minimum_convergence_bound: 800 #the minimum reward to consider convergence

#parameters for the environment
environment:
  observe_other_suppressant: false
  observe_other_power: false
  observe_burn_time: false #just adds to log doesn't add to observation
  show_bad_actions: false
  observe_relative_task_index: False


validation:
  seeds: [0,1,2,3,4]
  frequency: 40
  env_params: #overrides environment parameters
    terminate_on_no_fires: false
    observe_burn_time: true

#model parameters
epsilon_min: 0.001
epsilon_decay: .999
initial_epsilon: 0.9
gamma: 0.99
exp_ac_softhard_logits: false

both:
  ablation_ignore_task_nodes: true
  exp_linear_combination: false
  regularize: true
  beta: 0.01
  use_graph_norm: true
  node_feature_size: 5
  edge_feature_size: 5
  layer_type: 'gatv2'
  add_self_loops: true
  relu_slope: 0.1
  optimizer_fn: 'adam'
  psi: 0.005
  K: 15
  dropout_rate: 0.0
  num_heads: 2
  gradient_clip: 0.0
  
actor:
  fc_hyperedges: false
  lr: 0.009
  K: 0 #slow target update interval
  hidden_dim: 24
  number_of_layers: 3
  rotate_hyperedges: true
critic:
  fc_hyperedges: true
  lr: 0.01
  K: 0 #slow target update interval
  num_agents: 2
  hidden_dim: 24
  number_of_layers: 3
  gradient_clip: 0.0
