#parameters outside the model
training:
  environment_steps_per_update: 4 #dep (unused)
  seed: 16
  experiences_before_update: 1 # number of environment interactions before an update
  backups_per_update: 1 # number of update iterations per update step
  num_episodes: 20000 # number of episodes to run
  steps_per_checkpoint: 30 # number of steps before saving a checkpoint
  batch_size: 16 
  buffer_size: 1000
  wait_until_full: true # whether to wait until the buffer is full before starting training
  starting_state_strategy: 'random' # strategy for selecting the starting state during training
  observe_other_suppressant: false # whether to observe the other agent's suppressant 
  reprod_training: true # whether to use the reproduction training mode (determ convolutions)

validation:
  seeds: [0, 1, 2, 3, 4] 
  frequency: 10

#model parameters
epsilon_min: 0.001
epsilon_decay: .999
initial_epsilon: 0.9
gamma: 0.99
exp_ac_softhard_logits: false # use a stochastic pg style policy instead.

both: #parameters applied to both actor and critic
  exp_linear_combination: false # use a linear combination of output hyperedge features rather than pooling. Doesn't combine nodes just features.
  regularize: true #use our regularization loss term
  beta: 0.01
  use_graph_norm: true # include graph norm in the model
  node_feature_size: 5 # number of features per node
  edge_feature_size: 5 # number of features per hyperedge node
  layer_type: 'gatv2'
  add_self_loops: true
  relu_slope: 0.1
  optimizer_fn: 'adam'
  psi: 0.005
  K: 15
  dropout_rate: 0.0
  num_heads: 2
  gradient_clip: 0.0
actor: #actor specific
  lr: 0.009
  K: 0 #slow target update interval
  hidden_dim: 24
  number_of_layers: 3
critic: #critic specific
  lr: 0.01
  K: 0 #slow target update interval
  num_agents: 2
  hidden_dim: 24
  number_of_layers: 3
